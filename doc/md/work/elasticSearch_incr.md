## ElasticSearch中增量写入带来的空间消耗问题
### 问题场景：
1. 数据每天增量在50-70G左右
2. 在后台查询系统中，要求做到在能实时查询近1个月的数据(总量：1500G - 2100G)
3. 根据各个维度的类别查询，要求做到搜索数据返回时间在规定时间内响应
### 问题难点
- **由于后台系统需求是要有搜索功能**
```
几乎是整条数据的字段都有对应维度的搜索需求，所以没办法走ES+HBase的方案。采用的方案是将各个字段用标识符来标识，如1,2,3,4之类的数字来表示放入ES中。
尽可能的减少数据占用空间
```
- **每天数据在增加，磁盘空间有限**
```
由于需要一个月的数据，所以至少保证ES要有2T的空间。在有限的空间里面肯定需要做一个权衡，必须要在到达一定时间后删除部分数据。
删除数据方案有两种：
    1.使用ES的delete_by_query语法删除一个月前的数据。在测试环境中测试，数据量稍微写入2天的数据，然后测试删除，会发现删除速度特别的慢。
      ES会先查询再删除，如果删除和查询数据过大反而会拖慢整个ES集群。 ---不可取
    2.使用ES的直接删除表的功能，删除速度很快。可以根据不同日期建立不同的表，每次删除就删除当前日期之前的ES表。
      但是删除的时候要考虑一个月的查询条件，如6月1日删除5月1日之前的数据，6月2日删除5月2日之前的表。
```
- **在每日建一张查询表基础上，在查询的时候通过代码方式关联每张表的数据**
```
在解决删除表功能的基础上，考虑到要查询的数据是一个月内的，所以会查询很多张表，在代码实现上是难点。
但是可以利用ES的别名功能Alias 为所有相关的日期查询表添加同一个别名，在后台代码查询的时候就只需要查询别名就行
亲测：创建多张ES表，取同一个别名能有效查询
```
- **如此大的数据量，真正带来价值的数据多半出现在哪些查询类别上**
```
由于要满足尽快响应请求，在需求上要知道哪个维度的数据会是主要关注点，尽快预热数据。基本保证这个维度的数据查询是在内存中进行
ES写入数据后会先写入内存，再刷入磁盘中。在查询的时候也会先查询内存再查询磁盘，最终查询数据会停留在内存中。所以大多时候要提高ES的查询性能
无非就是加大内存，热数据预热到内存中。
```

### 汇总
1. 编写ES表创建接口，根据每天日期创建一张ES表(如:项目名_业务名_功能点_日期)。并为表取别名。最好是在前一天就创建好当天的ES表
2. 编写数据写入脚本，根据每天日期写入对应的表中
3. 后台查询接口使用别名查询的方式查询各个维度的数据
4. 删除表功能，删除30+n天前的数据，n天作为预留数据